{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b5141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content/PatchTST\n",
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "Python path head: ['/content/PatchTST/PatchTST_supervised', '/content/PatchTST/PatchTST_physics_integrated', '/content/PatchTST', '/', '/env/python']\n",
      "NumPy compatibility patch applied for np.Inf -> np.inf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set repository root path and change to it\n",
    "repo_root_path = '/content/PatchTST'\n",
    "os.chdir(repo_root_path)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Build clean sys.path with supervised ahead of physics to avoid utils shadowing\n",
    "supervised_path = os.path.join(repo_root_path, 'PatchTST_supervised')\n",
    "physics_path = os.path.join(repo_root_path, 'PatchTST_physics_integrated')\n",
    "new_paths = [p for p in [supervised_path, physics_path, repo_root_path] if p not in sys.path]\n",
    "sys.path = new_paths + sys.path  # prepend in desired order\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Python path head: {sys.path[:5]}\")\n",
    "\n",
    "# Numpy fix\n",
    "if not hasattr(np, 'Inf'):\n",
    "    np.Inf = np.inf\n",
    "    np.NaN = np.nan\n",
    "    np.NAN = np.nan\n",
    "    np.NINF = np.NINF if hasattr(np, 'NINF') else -np.inf\n",
    "    print(\"NumPy compatibility patch applied for np.Inf -> np.inf\")\n",
    "else:\n",
    "    print(\"NumPy already has np.Inf attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb06a0",
   "metadata": {},
   "source": [
    "## 1. Import Physics-Integrated PatchTST Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad84d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (models.py, line 595)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2414796797.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<cell line: 0>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from PatchTST_physics_integrated.models import PhysicsIntegratedPatchTST\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/content/PatchTST/PatchTST_physics_integrated/models.py\"\u001b[0;36m, line \u001b[0;32m595\u001b[0m\n\u001b[0;31m    )\\n            # Learnable mixing weight\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "from PatchTST_physics_integrated.config import PhysicsIntegratedConfig\n",
    "from PatchTST_physics_integrated.models import PhysicsIntegratedPatchTST\n",
    "from PatchTST_physics_integrated.training_utils import set_seed, get_target_indices, get_scheduler\n",
    "from PatchTST_physics_integrated.trainer import train_model\n",
    "from PatchTST_physics_integrated.evaluation import evaluate_model, evaluate_per_channel\n",
    "from PatchTST_physics_integrated.data_preprocessing import add_hour_of_day_features\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8b2fa",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2634f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "args = PhysicsIntegratedConfig()\n",
    "set_seed(args.random_seed)\n",
    "\n",
    "# Have low patience for trying attempts\n",
    "args.patience = 3\n",
    "args.use_cross_channel_encoder = True\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Input channels: {args.enc_in}\")\n",
    "print(f\"  Output channels: {args.c_out}\")\n",
    "print(f\"  Sequence length: {args.seq_len}\")\n",
    "print(f\"  Prediction length: {args.pred_len}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"  Patience: {args.patience}\")\n",
    "print(f\"\\nChannel Groups (sources → targets):\")\n",
    "for name, group in args.channel_groups.items():\n",
    "    indices = group['indices']\n",
    "    src_names = group.get('names', [])\n",
    "    output_indices = set(group.get('output_indices', []))\n",
    "    # Map output indices to names using the indices ordering\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(indices) if idx in output_indices] if src_names else []\n",
    "    print(f\"  {name}:\")\n",
    "    if src_names:\n",
    "        print(f\"    Sources: {', '.join(src_names)}\")\n",
    "    else:\n",
    "        print(f\"    Sources: (names not provided)\")\n",
    "    if tgt_names:\n",
    "        print(f\"    Targets: {', '.join(tgt_names)}\")\n",
    "    else:\n",
    "        print(f\"    Targets: (names not provided)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11772ef",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data (Add Hour Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hour-of-day features to dataset and apply max pooling to long channel features\n",
    "original_path = os.path.join(args.root_path, 'weather.csv')\n",
    "enhanced_path = os.path.join(args.root_path, args.data_path)\n",
    "\n",
    "# Long channel output indices: [12, 15, 11] = max. wv (m/s), raining (s), wv (m/s)\n",
    "long_channel_indices = [12, 15, 11]\n",
    "\n",
    "df_enhanced = add_hour_of_day_features(\n",
    "    original_path, \n",
    "    enhanced_path,\n",
    "    apply_pooling=True,\n",
    "    pool_channel_indices=long_channel_indices,\n",
    "    pool_kernel=args.long_channel_pool_kernel,\n",
    "    pool_stride=args.long_channel_pool_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660e01f",
   "metadata": {},
   "source": [
    "## 4. Load Data (Using PatchTST Data Providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to PatchTST_supervised directory for data_provider imports\n",
    "import importlib\n",
    "\n",
    "os.chdir(os.path.join(repo_root_path, 'PatchTST_supervised'))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Clear cached modules to avoid stale 'utils' shadowing\n",
    "for m in [\n",
    "    'utils', 'utils.timefeatures',\n",
    "    'data_provider', 'data_provider.data_loader', 'data_provider.data_factory'\n",
    "]:\n",
    "    if m in sys.modules:\n",
    "        sys.modules.pop(m, None)\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "\n",
    "os.chdir(repo_root_path)\n",
    "\n",
    "# Create data loaders\n",
    "train_data, train_loader = data_provider(args, 'train')\n",
    "val_data, val_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Val samples: {len(val_data)}\")\n",
    "print(f\"  Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907b34",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = PhysicsIntegratedPatchTST(args).float().to(device)\n",
    "\n",
    "# Get target indices and names\n",
    "target_indices, target_names = get_target_indices(args.channel_groups)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"  Target variables by group:\")\n",
    "for group, info in args.channel_groups.items():\n",
    "    output_indices = set(info.get('output_indices', []))\n",
    "    src_names = info.get('names', [])\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(info['indices']) if idx in output_indices] if src_names else []\n",
    "    print(f\"    {group}: {', '.join(tgt_names) if tgt_names else '(names not provided)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect full model architecture\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1e7e1",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e589083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Create scheduler (OneCycleLR as in baseline notebook)\n",
    "train_steps = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    steps_per_epoch=train_steps,\n",
    "    pct_start=args.pct_start,\n",
    "    epochs=args.train_epochs,\n",
    "    max_lr=args.learning_rate\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_path = args.checkpoints\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061ffe7",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232779d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    target_indices=target_indices,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c21149",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall losses\n",
    "axes[0].plot(history['train_losses'], label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(history['val_losses'], label='Validation Loss', marker='s', markersize=3)\n",
    "axes[0].plot(history['test_losses'], label='Test Loss', marker='^', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Target variable losses\n",
    "for target_name, losses in history['target_variable_losses'].items():\n",
    "    axes[1].plot(losses, label=target_name.capitalize(), marker='o', markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Target Variable Losses')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a637c",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6917189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = evaluate_model(model, test_loader, device, args)\n",
    "\n",
    "# Get per-channel metrics\n",
    "per_channel_metrics = evaluate_per_channel(\n",
    "    results['preds'],\n",
    "    results['trues'],\n",
    "    target_indices,\n",
    "    target_names\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Channel Metrics:\")\n",
    "for ch_name, metrics in per_channel_metrics.items():\n",
    "    print(f\"  {ch_name}:\")\n",
    "    print(f\"    MSE: {metrics['mse']:.7f}\")\n",
    "    print(f\"    MAE: {metrics['mae']:.7f}\")\n",
    "    print(f\"    RMSE: {metrics['rmse']:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa170ea3",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9635e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for target variables with random sample selection - showing original scale values\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Define the 6 features to visualize (in original scale)\n",
    "features_to_plot = [\n",
    "    'p (mbar)',           # air pressure\n",
    "    'T (degC)',           # temperature\n",
    "    'wv (m/s)',           # wind speed\n",
    "    'max. wv (m/s)',      # maximum wind speed\n",
    "    'rain (mm)',          # rainfall amount\n",
    "    'raining (s)'         # rainfall duration\n",
    "]\n",
    "\n",
    "# Get indices of these features in target_names\n",
    "plot_feature_indices = []\n",
    "plot_feature_names = []\n",
    "for feature in features_to_plot:\n",
    "    if feature in target_names:\n",
    "        idx = target_names.index(feature)\n",
    "        plot_feature_indices.append(idx)\n",
    "        plot_feature_names.append(feature)\n",
    "\n",
    "print(f\"Plotting {len(plot_feature_names)} features: {', '.join(plot_feature_names)}\")\n",
    "\n",
    "# Get scaler from test_data for inverse transform\n",
    "scaler = test_data.scaler\n",
    "num_full_features = scaler.mean_.shape[0]  # Total features in dataset\n",
    "\n",
    "# Select random samples to visualize\n",
    "num_samples = 3\n",
    "num_available = results['preds'].shape[0]\n",
    "sample_indices = np.random.choice(num_available, size=min(num_samples, num_available), replace=False)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(len(plot_feature_names), len(sample_indices), \n",
    "                         figsize=(5*len(sample_indices), 4*len(plot_feature_names)))\n",
    "if len(sample_indices) == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "if len(plot_feature_names) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "# Process each feature\n",
    "for plot_idx, (target_idx, feature_name) in enumerate(zip(plot_feature_indices, plot_feature_names)):\n",
    "    # Get the original channel index for this feature\n",
    "    ch_idx = target_indices[target_idx]\n",
    "    \n",
    "    for col, sample_idx in enumerate(sample_indices):\n",
    "        # Get normalized sequences\n",
    "        input_seq_norm = results['inputs'][sample_idx, :, target_idx]\n",
    "        true_seq_norm = results['trues'][sample_idx, :, target_idx]\n",
    "        pred_seq_norm = results['preds'][sample_idx, :, target_idx]\n",
    "        \n",
    "        # Convert input sequence to original scale\n",
    "        input_len = len(input_seq_norm)\n",
    "        input_full = np.zeros((input_len, num_full_features))\n",
    "        input_full[:, ch_idx] = input_seq_norm\n",
    "        input_seq = scaler.inverse_transform(input_full)[:, ch_idx]\n",
    "        \n",
    "        # Convert true sequence to original scale\n",
    "        pred_len = len(true_seq_norm)\n",
    "        true_full = np.zeros((pred_len, num_full_features))\n",
    "        true_full[:, ch_idx] = true_seq_norm\n",
    "        true_seq = scaler.inverse_transform(true_full)[:, ch_idx]\n",
    "        \n",
    "        # Convert predicted sequence to original scale\n",
    "        pred_full = np.zeros((pred_len, num_full_features))\n",
    "        pred_full[:, ch_idx] = pred_seq_norm\n",
    "        pred_seq = scaler.inverse_transform(pred_full)[:, ch_idx]\n",
    "        \n",
    "        # Plot\n",
    "        input_steps = np.arange(len(input_seq))\n",
    "        pred_steps = np.arange(len(input_seq), len(input_seq) + len(pred_seq))\n",
    "        \n",
    "        ax = axes[plot_idx, col]\n",
    "        ax.plot(input_steps, input_seq, 'b-', label='Input', linewidth=1.5)\n",
    "        ax.plot(pred_steps, true_seq, 'g-', label='Ground Truth', linewidth=1.5)\n",
    "        ax.plot(pred_steps, pred_seq, 'r--', label='Prediction', linewidth=1.5)\n",
    "        ax.axvline(x=len(input_seq)-1, color='gray', linestyle=':', linewidth=1.5)\n",
    "        ax.set_title(f'Sample {sample_idx} - {feature_name}', fontsize=10)\n",
    "        ax.set_xlabel('Time Step', fontsize=9)\n",
    "        ax.set_ylabel('Value (Original Scale)', fontsize=9)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the chart with datetime filename\n",
    "output_dir = os.path.join(repo_root_path, 'OutputCharts')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f'prediction_visualization_original_scale_{timestamp}.png'\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "print(f\"Chart saved to: {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
