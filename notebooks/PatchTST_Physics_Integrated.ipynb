{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b5141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content/PatchTST\n",
      "PyTorch Version: 2.8.0+cu126\n",
      "CUDA Available: True\n",
      "Python path head: ['/content/PatchTST/PatchTST_supervised', '/content/PatchTST/PatchTST_physics_integrated', '/content/PatchTST', '/', '/env/python']\n",
      "NumPy already has np.Inf attribute\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set repository root path and change to it\n",
    "repo_root_path = '/content/PatchTST'\n",
    "os.chdir(repo_root_path)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Build clean sys.path with supervised ahead of physics to avoid utils shadowing\n",
    "supervised_path = os.path.join(repo_root_path, 'PatchTST_supervised')\n",
    "physics_path = os.path.join(repo_root_path, 'PatchTST_physics_integrated')\n",
    "new_paths = [p for p in [supervised_path, physics_path, repo_root_path] if p not in sys.path]\n",
    "sys.path = new_paths + sys.path  # prepend in desired order\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Python path head: {sys.path[:5]}\")\n",
    "\n",
    "# Numpy fix\n",
    "if not hasattr(np, 'Inf'):\n",
    "    np.Inf = np.inf\n",
    "    np.NaN = np.nan\n",
    "    np.NAN = np.nan\n",
    "    np.NINF = np.NINF if hasattr(np, 'NINF') else -np.inf\n",
    "    print(\"NumPy compatibility patch applied for np.Inf -> np.inf\")\n",
    "else:\n",
    "    print(\"NumPy already has np.Inf attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb06a0",
   "metadata": {},
   "source": [
    "## 1. Import Physics-Integrated PatchTST Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad84d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "from PatchTST_physics_integrated.config import PhysicsIntegratedConfig\n",
    "from PatchTST_physics_integrated.models import PhysicsIntegratedPatchTST\n",
    "from PatchTST_physics_integrated.training_utils import set_seed, get_target_indices, get_scheduler\n",
    "from PatchTST_physics_integrated.trainer import train_model\n",
    "from PatchTST_physics_integrated.evaluation import evaluate_model, evaluate_per_channel\n",
    "from PatchTST_physics_integrated.data_preprocessing import add_hour_of_day_features\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8b2fa",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2634f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n",
      "\n",
      "Configuration:\n",
      "  Input channels: 23\n",
      "  Output channels: 21\n",
      "  Sequence length: 512\n",
      "  Prediction length: 336\n",
      "  Batch size: 64\n",
      "  Learning rate: 0.0001\n",
      "\n",
      "Channel Groups (sources → targets):\n",
      "  rain_predictors:\n",
      "    Sources: rain (mm), raining (s), rh (%), Tdew (degC), H2OC (mmol/mol), sh (g/kg), VPact (mbar), VPmax (mbar), VPdef (mbar), p (mbar), Tpot (K), rho (g/m**3), wv (m/s), max. wv (m/s), wd (deg), PAR (umol/m2/s), hour_sin, hour_cos\n",
      "    Targets: rain (mm), raining (s)\n",
      "  temperature_predictors:\n",
      "    Sources: T (degC), Tpot (K), Tdew (degC), PAR (umol/m2/s), p (mbar), rho (g/m**3), wv (m/s), max. wv (m/s), wd (deg), rh (%), sh (g/kg), H2OC (mmol/mol), VPact (mbar), VPmax (mbar), VPdef (mbar), hour_sin, hour_cos\n",
      "    Targets: T (degC), Tpot (K), Tdew (degC)\n",
      "  wind_predictors:\n",
      "    Sources: wv (m/s), max. wv (m/s), p (mbar), Tpot (K), T (degC), Tdew (degC), PAR (umol/m2/s), rh (%), sh (g/kg), H2OC (mmol/mol), VPact (mbar), VPmax (mbar), VPdef (mbar), rho (g/m**3), wd (deg), hour_sin, hour_cos\n",
      "    Targets: wv (m/s), max. wv (m/s)\n",
      "  other_variables:\n",
      "    Sources: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC), T (degC), Tpot (K), Tdew (degC), wv (m/s), max. wv (m/s), hour_sin, hour_cos\n",
      "    Targets: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC)\n"
     ]
    }
   ],
   "source": [
    "# Create configuration\n",
    "args = PhysicsIntegratedConfig()\n",
    "set_seed(args.random_seed)\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Input channels: {args.enc_in}\")\n",
    "print(f\"  Output channels: {args.c_out}\")\n",
    "print(f\"  Sequence length: {args.seq_len}\")\n",
    "print(f\"  Prediction length: {args.pred_len}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"  Patience: {args.patience}\")\n",
    "print(f\"\\nChannel Groups (sources → targets):\")\n",
    "for name, group in args.channel_groups.items():\n",
    "    indices = group['indices']\n",
    "    src_names = group.get('names', [])\n",
    "    output_indices = set(group.get('output_indices', []))\n",
    "    # Map output indices to names using the indices ordering\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(indices) if idx in output_indices] if src_names else []\n",
    "    print(f\"  {name}:\")\n",
    "    if src_names:\n",
    "        print(f\"    Sources: {', '.join(src_names)}\")\n",
    "    else:\n",
    "        print(f\"    Sources: (names not provided)\")\n",
    "    if tgt_names:\n",
    "        print(f\"    Targets: {', '.join(tgt_names)}\")\n",
    "    else:\n",
    "        print(f\"    Targets: (names not provided)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11772ef",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data (Add Hour Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ca1c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset from: ./datasets/weather/weather.csv\n",
      "Original shape: (52696, 22)\n",
      "Original columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT']\n",
      "\n",
      "New shape: (52696, 24)\n",
      "New columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT', 'hour_sin', 'hour_cos']\n",
      "\n",
      "Hour feature statistics:\n",
      "           hour_sin      hour_cos\n",
      "count  52696.000000  5.269600e+04\n",
      "mean      -0.000070  1.437736e-04\n",
      "std        0.707146  7.070808e-01\n",
      "min       -1.000000 -1.000000e+00\n",
      "25%       -0.707107 -7.071068e-01\n",
      "50%        0.000000  6.123234e-17\n",
      "75%        0.707107  7.071068e-01\n",
      "max        1.000000  1.000000e+00\n",
      "Original shape: (52696, 22)\n",
      "Original columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT']\n",
      "\n",
      "New shape: (52696, 24)\n",
      "New columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT', 'hour_sin', 'hour_cos']\n",
      "\n",
      "Hour feature statistics:\n",
      "           hour_sin      hour_cos\n",
      "count  52696.000000  5.269600e+04\n",
      "mean      -0.000070  1.437736e-04\n",
      "std        0.707146  7.070808e-01\n",
      "min       -1.000000 -1.000000e+00\n",
      "25%       -0.707107 -7.071068e-01\n",
      "50%        0.000000  6.123234e-17\n",
      "75%        0.707107  7.071068e-01\n",
      "max        1.000000  1.000000e+00\n",
      "\n",
      "✓ Enhanced dataset saved to: ./datasets/weather/weather_with_hour.csv\n",
      "\n",
      "✓ Enhanced dataset saved to: ./datasets/weather/weather_with_hour.csv\n"
     ]
    }
   ],
   "source": [
    "# Add hour-of-day features to dataset\n",
    "original_path = os.path.join(args.root_path, 'weather.csv')\n",
    "enhanced_path = os.path.join(args.root_path, args.data_path)\n",
    "\n",
    "df_enhanced = add_hour_of_day_features(original_path, enhanced_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660e01f",
   "metadata": {},
   "source": [
    "## 4. Load Data (Using PatchTST Data Providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "310d337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to: /content/PatchTST/PatchTST_supervised\n",
      "train 36040\n",
      "val 4935\n",
      "train 36040\n",
      "val 4935\n",
      "test 10204\n",
      "\n",
      "Data loaded:\n",
      "  Train samples: 36040\n",
      "  Val samples: 4935\n",
      "  Test samples: 10204\n",
      "test 10204\n",
      "\n",
      "Data loaded:\n",
      "  Train samples: 36040\n",
      "  Val samples: 4935\n",
      "  Test samples: 10204\n"
     ]
    }
   ],
   "source": [
    "# Change to PatchTST_supervised directory for data_provider imports\n",
    "import importlib\n",
    "\n",
    "os.chdir(os.path.join(repo_root_path, 'PatchTST_supervised'))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Clear cached modules to avoid stale 'utils' shadowing\n",
    "for m in [\n",
    "    'utils', 'utils.timefeatures',\n",
    "    'data_provider', 'data_provider.data_loader', 'data_provider.data_factory'\n",
    "]:\n",
    "    if m in sys.modules:\n",
    "        sys.modules.pop(m, None)\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "\n",
    "os.chdir(repo_root_path)\n",
    "\n",
    "# Create data loaders\n",
    "train_data, train_loader = data_provider(args, 'train')\n",
    "val_data, val_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Val samples: {len(val_data)}\")\n",
    "print(f\"  Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907b34",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7647f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model created:\n",
      "  Total parameters: 11,118,786\n",
      "  Target variables by group:\n",
      "    rain_predictors: rain (mm), raining (s)\n",
      "    temperature_predictors: T (degC), Tpot (K), Tdew (degC)\n",
      "    wind_predictors: wv (m/s), max. wv (m/s)\n",
      "    other_variables: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = PhysicsIntegratedPatchTST(args).float().to(device)\n",
    "\n",
    "# Get target indices and names\n",
    "target_indices, target_names = get_target_indices(args.channel_groups)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"  Target variables by group:\")\n",
    "for group, info in args.channel_groups.items():\n",
    "    output_indices = set(info.get('output_indices', []))\n",
    "    src_names = info.get('names', [])\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(info['indices']) if idx in output_indices] if src_names else []\n",
    "    print(f\"    {group}: {', '.join(tgt_names) if tgt_names else '(names not provided)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8ebf75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture:\n",
      "\n",
      "PhysicsIntegratedPatchTST(\n",
      "  (revin_layer): RevIN()\n",
      "  (encoders): ModuleDict(\n",
      "    (rain_predictors): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 12))\n",
      "      (W_P): Linear(in_features=24, out_features=128, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0-17): 18 x CustomMultiheadAttention(\n",
      "          (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (sdp_attn): _ScaledDotProductAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.2, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_head): Sequential(\n",
      "        (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "        (1): Linear(in_features=5376, out_features=336, bias=True)\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (temperature_predictors): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 18))\n",
      "      (W_P): Linear(in_features=36, out_features=128, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0-16): 17 x CustomMultiheadAttention(\n",
      "          (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (sdp_attn): _ScaledDotProductAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.2, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_head): Sequential(\n",
      "        (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "        (1): Linear(in_features=3584, out_features=336, bias=True)\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (wind_predictors): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 16))\n",
      "      (W_P): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0-16): 17 x CustomMultiheadAttention(\n",
      "          (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (sdp_attn): _ScaledDotProductAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.2, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_head): Sequential(\n",
      "        (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "        (1): Linear(in_features=4096, out_features=336, bias=True)\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (other_variables): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 12))\n",
      "      (W_P): Linear(in_features=24, out_features=128, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0-20): 21 x CustomMultiheadAttention(\n",
      "          (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (sdp_attn): _ScaledDotProductAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.2, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_head): Sequential(\n",
      "        (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "        (1): Linear(in_features=5376, out_features=336, bias=True)\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_group_attn): CrossGroupAttention(\n",
      "    (channel_proj): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (cross_attn): CustomMultiheadAttention(\n",
      "      (W_Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (W_K): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (W_V): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (sdp_attn): _ScaledDotProductAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (dropout_attn): Dropout(p=0.2, inplace=False)\n",
      "      (norm_attn): Sequential(\n",
      "        (0): Transpose()\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Transpose()\n",
      "      )\n",
      "    )\n",
      "    (ffn): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (norm_ffn): Sequential(\n",
      "      (0): Transpose()\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Transpose()\n",
      "    )\n",
      "    (output_proj): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect full model architecture\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1e7e1",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e589083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Create scheduler (OneCycleLR as in baseline notebook)\n",
    "train_steps = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    steps_per_epoch=train_steps,\n",
    "    pct_start=args.pct_start,\n",
    "    epochs=args.train_epochs,\n",
    "    max_lr=args.learning_rate\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_path = args.checkpoints\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061ffe7",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232779d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Physics-Integrated PatchTST Training...\n",
      "Checkpoint path: ./checkpoints_physics_integrated\n",
      "======================================================================\n",
      "Input channels: 23 (21 weather + 2 hour)\n",
      "Output channels: 21 (21 weather only)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Physics Groups (with integrated hour features):\n",
      "  rain_predictors: 2 weather + hour → patch=24 , stride=12\n",
      "  temperature_predictors: 3 weather + hour → patch=36 , stride=18\n",
      "  wind_predictors: 2 weather + hour → patch=32 , stride=16\n",
      "  other_variables: 14 weather + hour → patch=24 , stride=12\n",
      "======================================================================\n",
      "\n",
      "Epoch 1/100 | Time: 131.99s\n",
      "  Train Loss: 0.6351745 | Val Loss: 0.5902920 | Test Loss: 0.3365551\n",
      "  Epoch duration: 131.99 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.7759459\n",
      "    Temperature: 0.6138863\n",
      "    Wind: 0.5263355\n",
      "Validation loss decreased (inf --> 0.590292). Saving model ...\n",
      "\n",
      "Epoch 1/100 | Time: 131.99s\n",
      "  Train Loss: 0.6351745 | Val Loss: 0.5902920 | Test Loss: 0.3365551\n",
      "  Epoch duration: 131.99 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.7759459\n",
      "    Temperature: 0.6138863\n",
      "    Wind: 0.5263355\n",
      "Validation loss decreased (inf --> 0.590292). Saving model ...\n",
      "\n",
      "Epoch 2/100 | Time: 134.89s\n",
      "  Train Loss: 0.5568336 | Val Loss: 0.5403061 | Test Loss: 0.3074867\n",
      "  Epoch duration: 134.89 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.7221845\n",
      "    Temperature: 0.5423730\n",
      "    Wind: 0.4131735\n",
      "Validation loss decreased (0.590292 --> 0.540306). Saving model ...\n",
      "\n",
      "Epoch 2/100 | Time: 134.89s\n",
      "  Train Loss: 0.5568336 | Val Loss: 0.5403061 | Test Loss: 0.3074867\n",
      "  Epoch duration: 134.89 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.7221845\n",
      "    Temperature: 0.5423730\n",
      "    Wind: 0.4131735\n",
      "Validation loss decreased (0.590292 --> 0.540306). Saving model ...\n",
      "\n",
      "Epoch 3/100 | Time: 130.89s\n",
      "  Train Loss: 0.5286324 | Val Loss: 0.5170494 | Test Loss: 0.2940885\n",
      "  Epoch duration: 130.89 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6832493\n",
      "    Temperature: 0.5166121\n",
      "    Wind: 0.3920460\n",
      "Validation loss decreased (0.540306 --> 0.517049). Saving model ...\n",
      "\n",
      "Epoch 3/100 | Time: 130.89s\n",
      "  Train Loss: 0.5286324 | Val Loss: 0.5170494 | Test Loss: 0.2940885\n",
      "  Epoch duration: 130.89 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6832493\n",
      "    Temperature: 0.5166121\n",
      "    Wind: 0.3920460\n",
      "Validation loss decreased (0.540306 --> 0.517049). Saving model ...\n",
      "\n",
      "Epoch 4/100 | Time: 135.02s\n",
      "  Train Loss: 0.5152074 | Val Loss: 0.5054932 | Test Loss: 0.2883551\n",
      "  Epoch duration: 135.02 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6672483\n",
      "    Temperature: 0.5037082\n",
      "    Wind: 0.3804154\n",
      "Validation loss decreased (0.517049 --> 0.505493). Saving model ...\n",
      "\n",
      "Epoch 4/100 | Time: 135.02s\n",
      "  Train Loss: 0.5152074 | Val Loss: 0.5054932 | Test Loss: 0.2883551\n",
      "  Epoch duration: 135.02 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6672483\n",
      "    Temperature: 0.5037082\n",
      "    Wind: 0.3804154\n",
      "Validation loss decreased (0.517049 --> 0.505493). Saving model ...\n",
      "\n",
      "Epoch 5/100 | Time: 130.45s\n",
      "  Train Loss: 0.5072156 | Val Loss: 0.4969041 | Test Loss: 0.2852664\n",
      "  Epoch duration: 130.45 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6600684\n",
      "    Temperature: 0.4966402\n",
      "    Wind: 0.3702260\n",
      "Validation loss decreased (0.505493 --> 0.496904). Saving model ...\n",
      "\n",
      "Epoch 5/100 | Time: 130.45s\n",
      "  Train Loss: 0.5072156 | Val Loss: 0.4969041 | Test Loss: 0.2852664\n",
      "  Epoch duration: 130.45 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6600684\n",
      "    Temperature: 0.4966402\n",
      "    Wind: 0.3702260\n",
      "Validation loss decreased (0.505493 --> 0.496904). Saving model ...\n",
      "\n",
      "Epoch 6/100 | Time: 134.11s\n",
      "  Train Loss: 0.5018379 | Val Loss: 0.4928655 | Test Loss: 0.2839141\n",
      "  Epoch duration: 134.11 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6565161\n",
      "    Temperature: 0.4921135\n",
      "    Wind: 0.3617464\n",
      "Validation loss decreased (0.496904 --> 0.492866). Saving model ...\n",
      "\n",
      "Epoch 6/100 | Time: 134.11s\n",
      "  Train Loss: 0.5018379 | Val Loss: 0.4928655 | Test Loss: 0.2839141\n",
      "  Epoch duration: 134.11 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6565161\n",
      "    Temperature: 0.4921135\n",
      "    Wind: 0.3617464\n",
      "Validation loss decreased (0.496904 --> 0.492866). Saving model ...\n",
      "\n",
      "Epoch 7/100 | Time: 133.24s\n",
      "  Train Loss: 0.4976497 | Val Loss: 0.4937426 | Test Loss: 0.2832569\n",
      "  Epoch duration: 133.24 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6545451\n",
      "    Temperature: 0.4886595\n",
      "    Wind: 0.3542396\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch 7/100 | Time: 133.24s\n",
      "  Train Loss: 0.4976497 | Val Loss: 0.4937426 | Test Loss: 0.2832569\n",
      "  Epoch duration: 133.24 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6545451\n",
      "    Temperature: 0.4886595\n",
      "    Wind: 0.3542396\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch 8/100 | Time: 132.70s\n",
      "  Train Loss: 0.4939360 | Val Loss: 0.4888822 | Test Loss: 0.2828150\n",
      "  Epoch duration: 132.70 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6528700\n",
      "    Temperature: 0.4854405\n",
      "    Wind: 0.3477452\n",
      "Validation loss decreased (0.492866 --> 0.488882). Saving model ...\n",
      "\n",
      "Epoch 8/100 | Time: 132.70s\n",
      "  Train Loss: 0.4939360 | Val Loss: 0.4888822 | Test Loss: 0.2828150\n",
      "  Epoch duration: 132.70 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6528700\n",
      "    Temperature: 0.4854405\n",
      "    Wind: 0.3477452\n",
      "Validation loss decreased (0.492866 --> 0.488882). Saving model ...\n",
      "\n",
      "Epoch 9/100 | Time: 132.31s\n",
      "  Train Loss: 0.4909185 | Val Loss: 0.4876567 | Test Loss: 0.2821610\n",
      "  Epoch duration: 132.31 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6521229\n",
      "    Temperature: 0.4827694\n",
      "    Wind: 0.3419379\n",
      "Validation loss decreased (0.488882 --> 0.487657). Saving model ...\n",
      "\n",
      "Epoch 9/100 | Time: 132.31s\n",
      "  Train Loss: 0.4909185 | Val Loss: 0.4876567 | Test Loss: 0.2821610\n",
      "  Epoch duration: 132.31 seconds\n",
      "  Target Variable Losses:\n",
      "    Rain: 0.6521229\n",
      "    Temperature: 0.4827694\n",
      "    Wind: 0.3419379\n",
      "Validation loss decreased (0.488882 --> 0.487657). Saving model ...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    target_indices=target_indices,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aefda7",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c21149",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall losses\n",
    "axes[0].plot(history['train_losses'], label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(history['val_losses'], label='Validation Loss', marker='s', markersize=3)\n",
    "axes[0].plot(history['test_losses'], label='Test Loss', marker='^', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Target variable losses\n",
    "for target_name, losses in history['target_variable_losses'].items():\n",
    "    axes[1].plot(losses, label=target_name.capitalize(), marker='o', markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Target Variable Losses')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a637c",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6917189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = evaluate_model(model, test_loader, device, args)\n",
    "\n",
    "# Get per-channel metrics\n",
    "per_channel_metrics = evaluate_per_channel(\n",
    "    results['preds'],\n",
    "    results['trues'],\n",
    "    target_indices,\n",
    "    target_names\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Channel Metrics:\")\n",
    "for ch_name, metrics in per_channel_metrics.items():\n",
    "    print(f\"  {ch_name}:\")\n",
    "    print(f\"    MSE: {metrics['mse']:.7f}\")\n",
    "    print(f\"    MAE: {metrics['mae']:.7f}\")\n",
    "    print(f\"    RMSE: {metrics['rmse']:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa170ea3",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9635e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for target variables\n",
    "num_samples = 3\n",
    "fig, axes = plt.subplots(len(target_indices), num_samples, figsize=(5*num_samples, 4*len(target_indices)))\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i, (ch_idx, ch_name) in enumerate(zip(target_indices, target_names)):\n",
    "    for j in range(num_samples):\n",
    "        input_seq = results['inputs'][j, :, ch_idx]\n",
    "        true_seq = results['trues'][j, :, ch_idx]\n",
    "        pred_seq = results['preds'][j, :, ch_idx]\n",
    "        \n",
    "        input_steps = np.arange(len(input_seq))\n",
    "        pred_steps = np.arange(len(input_seq), len(input_seq) + len(pred_seq))\n",
    "        \n",
    "        ax = axes[i, j]\n",
    "        ax.plot(input_steps, input_seq, 'b-', label='Input')\n",
    "        ax.plot(pred_steps, true_seq, 'g-', label='Ground Truth')\n",
    "        ax.plot(pred_steps, pred_seq, 'r--', label='Prediction')\n",
    "        ax.axvline(x=len(input_seq)-1, color='gray', linestyle=':', linewidth=1.5)\n",
    "        ax.set_title(f'Sample {j+1} - {ch_name}')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
