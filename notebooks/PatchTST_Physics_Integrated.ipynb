{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379f3b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content/PatchTST\n",
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "Python path head: ['/content/PatchTST/PatchTST_supervised', '/content/PatchTST/PatchTST_physics_integrated', '/content/PatchTST', '/content/PatchTST/notebooks', '/env/python']\n",
      "NumPy compatibility patch applied for np.Inf -> np.inf\n",
      "✓ All modules imported successfully\n",
      "Random seed set to: 2021\n",
      "Configuration:\n",
      "  seq_len (look back): 336\n",
      "  pred_len (forecast): 336\n",
      "  batch_size: 16\n",
      "  train_epochs: 15\n",
      "  enc_in (features): 24\n",
      "  c_out (output features): 24\n",
      "\n",
      "Channel groups output all 24 features\n",
      "Deleted cached file: ./datasets/weather/weather_with_hour.csv\n",
      "Loading original dataset from: ./datasets/weather/weather.csv\n",
      "Original shape: (52696, 22)\n",
      "Original columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT']\n",
      "\n",
      "New shape: (52696, 24)\n",
      "New columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT', 'hour_sin', 'hour_cos']\n",
      "\n",
      "Hour feature statistics:\n",
      "           hour_sin      hour_cos\n",
      "count  52696.000000  5.269600e+04\n",
      "mean      -0.000070  1.437736e-04\n",
      "std        0.707146  7.070808e-01\n",
      "min       -1.000000 -1.000000e+00\n",
      "25%       -0.707107 -7.071068e-01\n",
      "50%        0.000000  6.123234e-17\n",
      "75%        0.707107  7.071068e-01\n",
      "max        1.000000  1.000000e+00\n",
      "\n",
      "✓ Enhanced dataset saved to: ./datasets/weather/weather_with_hour.csv\n",
      "\n",
      "✓ Data regenerated from original source\n",
      "  Original: ./datasets/weather/weather.csv\n",
      "  Enhanced: ./datasets/weather/weather_with_hour.csv\n",
      "  Rows: 52696, Columns: 24\n",
      "  Max pooling: DISABLED\n",
      "Changed to: /content/PatchTST/PatchTST_supervised\n",
      "train 36216\n",
      "val 4935\n",
      "test 10204\n",
      "\n",
      "Data loaded:\n",
      "  Train samples: 36216\n",
      "  Val samples: 4935\n",
      "  Test samples: 10204\n",
      "\n",
      "Data normalization:\n",
      "  Mean shape: (23,)\n",
      "  Std shape: (23,)\n",
      "\n",
      "============================================================\n",
      "TEST DATA VERIFICATION (Physics-Integrated)\n",
      "============================================================\n",
      "Test loader info:\n",
      "  Total batches: 637\n",
      "  Batch size: 16\n",
      "  Input shape (batch_x): torch.Size([16, 336, 23])\n",
      "  Output shape (batch_y): torch.Size([16, 384, 23])\n",
      "  Features in test_data: 23\n",
      "\n",
      "Column order AFTER data loader reordering (excluding 'date'):\n",
      "  Total features: 23\n",
      "  First 20: ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)']\n",
      "  Features 20-22: ['hour_sin', 'hour_cos', 'OT']\n",
      "  Last feature: OT\n",
      "\n",
      "First test sample statistics (First 20 features):\n",
      "  Mean: -0.048374\n",
      "  Std: 0.554955\n",
      "  Min: -0.926592\n",
      "  Max: 1.544642\n",
      "  Sum: -0.967477\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python '/content/PatchTST/notebooks/MutiScalePatchTST.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set repository root path and change to it\n",
    "repo_root_path = '/content/PatchTST'\n",
    "os.chdir(repo_root_path)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Build clean sys.path with supervised ahead of physics to avoid utils shadowing\n",
    "supervised_path = os.path.join(repo_root_path, 'PatchTST_supervised')\n",
    "physics_path = os.path.join(repo_root_path, 'PatchTST_physics_integrated')\n",
    "new_paths = [p for p in [supervised_path, physics_path, repo_root_path] if p not in sys.path]\n",
    "sys.path = new_paths + sys.path  # prepend in desired order\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Python path head: {sys.path[:5]}\")\n",
    "\n",
    "# Numpy fix\n",
    "if not hasattr(np, 'Inf'):\n",
    "    np.Inf = np.inf\n",
    "    np.NaN = np.nan\n",
    "    np.NAN = np.nan\n",
    "    np.NINF = np.NINF if hasattr(np, 'NINF') else -np.inf\n",
    "    print(\"NumPy compatibility patch applied for np.Inf -> np.inf\")\n",
    "else:\n",
    "    print(\"NumPy already has np.Inf attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb06a0",
   "metadata": {},
   "source": [
    "## 1. Import Physics-Integrated PatchTST Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad84d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MSVLPatchTST.config import PhysicsIntegratedConfig\n",
    "from MSVLPatchTST.models import PhysicsIntegratedPatchTST\n",
    "from MSVLPatchTST.training_utils import set_seed, get_target_indices, get_scheduler\n",
    "from MSVLPatchTST.trainer import train_model\n",
    "from MSVLPatchTST.evaluation import evaluate_model, evaluate_per_channel\n",
    "from MSVLPatchTST.data_preprocessing import add_hour_of_day_features\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8b2fa",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2634f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "args = PhysicsIntegratedConfig()\n",
    "args.random_seed = 2021\n",
    "set_seed(args.random_seed)\n",
    "\n",
    "# Override key parameters for comparison with DLinear baseline\n",
    "args.seq_len = 336       # Look back window (same as DLinear baseline)\n",
    "args.pred_len = 336      # Prediction horizon (same as DLinear baseline)\n",
    "args.patience = 3\n",
    "args.train_epochs = 15\n",
    "args.batch_size = 16     # Same batch size as DLinear baseline\n",
    "args.use_cross_channel_encoder = False\n",
    "args.use_cross_group_attention = True\n",
    "\n",
    "# CRITICAL: Make model predict ALL 22 features (not just 7 targets)\n",
    "# Update channel groups to output all indices\n",
    "for group_name in args.channel_groups:\n",
    "    # Set output_indices to all indices (0-21 for 22 features)\n",
    "    args.channel_groups[group_name]['output_indices'] = list(range(22))\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  seq_len (look back): {args.seq_len}\")\n",
    "print(f\"  pred_len (forecast): {args.pred_len}\")\n",
    "print(f\"  batch_size: {args.batch_size}\")\n",
    "print(f\"  train_epochs: {args.train_epochs}\")\n",
    "print(f\"  enc_in (features): {args.enc_in}\")\n",
    "print(f\"  c_out (output features): {args.c_out}\")\n",
    "print(f\"\\nChannel groups output all 22 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11772ef",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data (Add Hour Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hour-of-day features to dataset\n",
    "# ALWAYS regenerate from original source - no caching\n",
    "original_path = os.path.join(args.root_path, 'weather.csv')\n",
    "enhanced_path = os.path.join(args.root_path, args.data_path)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2021)\n",
    "\n",
    "# ALWAYS delete cached file to force regeneration from original source\n",
    "if os.path.exists(enhanced_path):\n",
    "    os.remove(enhanced_path)\n",
    "    print(f\"Deleted cached file: {enhanced_path}\")\n",
    "\n",
    "# Regenerate with NO max pooling\n",
    "df_enhanced = add_hour_of_day_features(\n",
    "    original_path, \n",
    "    enhanced_path,\n",
    "    apply_pooling=False  # NO max pooling\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data regenerated from original source\")\n",
    "print(f\"  Original: {original_path}\")\n",
    "print(f\"  Enhanced: {enhanced_path}\")\n",
    "print(f\"  Rows: {len(df_enhanced)}, Columns: {len(df_enhanced.columns)}\")\n",
    "print(f\"  Max pooling: DISABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660e01f",
   "metadata": {},
   "source": [
    "## 4. Load Data (Using PatchTST Data Providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to PatchTST_supervised directory for data_provider imports\n",
    "import importlib\n",
    "\n",
    "os.chdir(os.path.join(repo_root_path, 'PatchTST_supervised'))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Clear cached modules to avoid stale 'utils' shadowing\n",
    "for m in [\n",
    "    'utils', 'utils.timefeatures',\n",
    "    'data_provider', 'data_provider.data_loader', 'data_provider.data_factory'\n",
    "]:\n",
    "    if m in sys.modules:\n",
    "        sys.modules.pop(m, None)\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "\n",
    "os.chdir(repo_root_path)\n",
    "\n",
    "# Set seed before data loading to ensure reproducible splits\n",
    "import random\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "torch.manual_seed(2021)\n",
    "\n",
    "# Create data loaders\n",
    "train_data, train_loader = data_provider(args, 'train')\n",
    "val_data, val_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Val samples: {len(val_data)}\")\n",
    "print(f\"  Test samples: {len(test_data)}\")\n",
    "\n",
    "# Verify data normalization parameters match\n",
    "if hasattr(train_data, 'scaler'):\n",
    "    print(f\"\\nData normalization:\")\n",
    "    print(f\"  Mean shape: {train_data.scaler.mean_.shape if hasattr(train_data.scaler, 'mean_') else 'N/A'}\")\n",
    "    print(f\"  Std shape: {train_data.scaler.scale_.shape if hasattr(train_data.scaler, 'scale_') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a572da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify test data for comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST DATA VERIFICATION (Physics-Integrated)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a sample batch to inspect\n",
    "sample_batch = next(iter(test_loader))\n",
    "batch_x, batch_y, batch_x_mark, batch_y_mark = sample_batch\n",
    "\n",
    "print(f\"Test loader info:\")\n",
    "print(f\"  Total batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size: {batch_x.shape[0]}\")\n",
    "print(f\"  Input shape (batch_x): {batch_x.shape}\")\n",
    "print(f\"  Output shape (batch_y): {batch_y.shape}\")\n",
    "print(f\"  Features in test_data: {test_data.data_x.shape[1] if hasattr(test_data, 'data_x') else 'N/A'}\")\n",
    "\n",
    "# Print first sample statistics for verification (FIRST 20 WEATHER FEATURES ONLY)\n",
    "if hasattr(test_data, 'data_x') and len(test_data.data_x) > 0:\n",
    "    first_sample_20 = test_data.data_x[0, :20]  # First 20 weather features (indices 0-19, exclude hour_sin, hour_cos at 20-21)\n",
    "    print(f\"\\nFirst test sample statistics (First 20 weather features only):\")\n",
    "    print(f\"  Mean: {first_sample_20.mean():.6f}\")\n",
    "    print(f\"  Std: {first_sample_20.std():.6f}\")\n",
    "    print(f\"  Min: {first_sample_20.min():.6f}\")\n",
    "    print(f\"  Max: {first_sample_20.max():.6f}\")\n",
    "    print(f\"  Sum: {first_sample_20.sum():.6f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECT CSV COMPARISON - Verify raw data sources match\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW CSV DATA COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the actual CSV files\n",
    "original_csv = pd.read_csv(original_path)\n",
    "enhanced_csv = pd.read_csv(enhanced_path)\n",
    "\n",
    "# Drop OT from original if present\n",
    "if 'OT' in original_csv.columns:\n",
    "    original_csv = original_csv.drop(columns=['OT'])\n",
    "\n",
    "# For enhanced, we have 20 weather + 2 hour features\n",
    "# Compare first 20 weather features (columns 1-20, excluding 'date')\n",
    "original_weather = original_csv.iloc[:, 1:21].values  # Skip 'date' column\n",
    "enhanced_weather = enhanced_csv.iloc[:, 1:21].values  # Skip 'date' column\n",
    "\n",
    "print(f\"\\nOriginal CSV shape: {original_csv.shape}\")\n",
    "print(f\"Enhanced CSV shape: {enhanced_csv.shape}\")\n",
    "\n",
    "# Compare a specific row (e.g., row 10000 to avoid boundary effects)\n",
    "test_row_idx = 10000\n",
    "original_row = original_weather[test_row_idx]\n",
    "enhanced_row = enhanced_weather[test_row_idx]\n",
    "\n",
    "print(f\"\\nComparing row {test_row_idx} (first 20 weather features):\")\n",
    "print(f\"Original: Mean={original_row.mean():.6f}, Std={original_row.std():.6f}, Sum={original_row.sum():.6f}\")\n",
    "print(f\"Enhanced: Mean={enhanced_row.mean():.6f}, Std={enhanced_row.std():.6f}, Sum={enhanced_row.sum():.6f}\")\n",
    "print(f\"Arrays equal: {np.allclose(original_row, enhanced_row)}\")\n",
    "\n",
    "# Check if all weather features match across entire dataset\n",
    "weather_match = np.allclose(original_weather, enhanced_weather)\n",
    "print(f\"\\nAll weather features match: {weather_match}\")\n",
    "\n",
    "if not weather_match:\n",
    "    print(\"WARNING: Weather features differ between original and enhanced CSV!\")\n",
    "    diff_count = np.sum(~np.isclose(original_weather, enhanced_weather))\n",
    "    print(f\"Number of different values: {diff_count} out of {original_weather.size}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db266672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP DATA INSPECTION - Find exact test sample positions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA NORMALIZATION & SPLIT INSPECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if scaler exists and display normalization parameters\n",
    "if hasattr(test_data, 'scaler') and test_data.scaler is not None:\n",
    "    scaler = test_data.scaler\n",
    "    print(f\"\\nNormalization parameters:\")\n",
    "    if hasattr(scaler, 'mean_'):\n",
    "        print(f\"  Mean (first 5): {scaler.mean_[:5]}\")\n",
    "        print(f\"  Std (first 5): {scaler.scale_[:5]}\")\n",
    "    print(f\"  Scaler type: {type(scaler).__name__}\")\n",
    "else:\n",
    "    print(\"\\nNo scaler found - data may not be normalized\")\n",
    "\n",
    "# Check train/val/test split information\n",
    "if hasattr(test_data, 'border1s') and hasattr(test_data, 'border2s'):\n",
    "    print(f\"\\nData split borders:\")\n",
    "    print(f\"  Test start index (border1s): {test_data.border1s}\")\n",
    "    print(f\"  Test end index (border2s): {test_data.border2s}\")\n",
    "    print(f\"  Test data length: {test_data.border2s - test_data.border1s}\")\n",
    "\n",
    "# Load the RAW CSV data (before normalization) and find what the test set sees\n",
    "enhanced_csv_raw = pd.read_csv(enhanced_path)\n",
    "if 'OT' in enhanced_csv_raw.columns:\n",
    "    enhanced_csv_raw = enhanced_csv_raw.drop(columns=['OT'])\n",
    "\n",
    "# Get test data raw values (before normalization)\n",
    "if hasattr(test_data, 'border1s'):\n",
    "    test_start_idx = test_data.border1s\n",
    "    # The first test sample corresponds to this row in the CSV\n",
    "    first_test_row_idx = test_start_idx\n",
    "    raw_first_test = enhanced_csv_raw.iloc[first_test_row_idx, 1:21].values  # First 20 weather features\n",
    "    \n",
    "    print(f\"\\nFirst test sample (CSV row {first_test_row_idx}, BEFORE normalization):\")\n",
    "    print(f\"  Mean: {raw_first_test.mean():.6f}\")\n",
    "    print(f\"  Std: {raw_first_test.std():.6f}\")\n",
    "    print(f\"  Sum: {raw_first_test.sum():.6f}\")\n",
    "    \n",
    "    # Now check the normalized version\n",
    "    if hasattr(test_data, 'data_x') and len(test_data.data_x) > 0:\n",
    "        normalized_first_test = test_data.data_x[0, :20]\n",
    "        print(f\"\\nFirst test sample (AFTER normalization):\")\n",
    "        print(f\"  Mean: {normalized_first_test.mean():.6f}\")\n",
    "        print(f\"  Std: {normalized_first_test.std():.6f}\")\n",
    "        print(f\"  Sum: {normalized_first_test.sum():.6f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ea2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ground truth for first test sample\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Features to visualize\n",
    "features_to_plot = [\n",
    "    'p (mbar)',           # air pressure\n",
    "    'T (degC)',           # temperature\n",
    "    'wv (m/s)',           # wind speed\n",
    "    'max. wv (m/s)',      # maximum wind speed\n",
    "    'rain (mm)',          # rainfall amount\n",
    "    'raining (s)'         # rainfall duration\n",
    "]\n",
    "\n",
    "# All 22 feature names (20 weather + 2 hour)\n",
    "all_feature_names = [\n",
    "    'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', \n",
    "    'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', \n",
    "    'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', \n",
    "    'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m²)', \n",
    "    'PAR (µmol/m²/s)', 'max. PAR (µmol/m²/s)', 'Tlog (degC)', 'hour_sin', 'hour_cos'\n",
    "]\n",
    "\n",
    "# Get first test sample (using same seed as inference for consistency)\n",
    "np.random.seed(2021)\n",
    "if hasattr(test_data, 'data_x') and len(test_data.data_x) > 0:\n",
    "    # Get the first sample's ground truth sequence\n",
    "    first_sample = test_data.data_x[0]  # Shape: (22,) - single timestep\n",
    "    \n",
    "    # For visualization, we'll show a longer sequence - get from batch\n",
    "    sample_gt = batch_y[0].cpu().numpy()  # Shape: (seq_len, 22)\n",
    "    \n",
    "    print(f\"\\nVisualizing ground truth from test data\")\n",
    "    print(f\"Ground truth shape: {sample_gt.shape}\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each feature\n",
    "    for i, feature_name in enumerate(features_to_plot):\n",
    "        feature_idx = all_feature_names.index(feature_name)\n",
    "        ax = axes[i]\n",
    "        \n",
    "        time_steps = np.arange(len(sample_gt))\n",
    "        ax.plot(time_steps, sample_gt[:, feature_idx], \n",
    "                linewidth=2, color='blue', alpha=0.8)\n",
    "        \n",
    "        ax.set_title(feature_name, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time Step', fontsize=10)\n",
    "        ax.set_ylabel('Value', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Test Data Ground Truth - First Sample', \n",
    "                 fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Ground truth visualization complete\")\n",
    "else:\n",
    "    print(\"Test data not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907b34",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = PhysicsIntegratedPatchTST(args).float().to(device)\n",
    "\n",
    "# Get target indices and names\n",
    "target_indices, target_names = get_target_indices(args.channel_groups)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"  Target variables by group:\")\n",
    "for group, info in args.channel_groups.items():\n",
    "    output_indices = set(info.get('output_indices', []))\n",
    "    src_names = info.get('names', [])\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(info['indices']) if idx in output_indices] if src_names else []\n",
    "    print(f\"    {group}: {', '.join(tgt_names) if tgt_names else '(names not provided)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect full model architecture\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1e7e1",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e589083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Create scheduler (OneCycleLR as in baseline notebook)\n",
    "train_steps = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    steps_per_epoch=train_steps,\n",
    "    pct_start=args.pct_start,\n",
    "    epochs=args.train_epochs,\n",
    "    max_lr=args.learning_rate\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_path = args.checkpoints\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061ffe7",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232779d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    target_indices=target_indices,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c33888",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall losses\n",
    "axes[0].plot(history['train_losses'], label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(history['val_losses'], label='Validation Loss', marker='s', markersize=3)\n",
    "axes[0].plot(history['test_losses'], label='Test Loss', marker='^', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Target variable losses\n",
    "for target_name, losses in history['target_variable_losses'].items():\n",
    "    axes[1].plot(losses, label=target_name.capitalize(), marker='o', markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Target Variable Losses')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f4890",
   "metadata": {},
   "source": [
    "## 9. Load Best Checkpoint and Evaluate MSE/MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint for inference\n",
    "checkpoint_file = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
    "print(f\"Loading checkpoint from: {checkpoint_file}\")\n",
    "\n",
    "# The checkpoint is saved as a direct state_dict\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "print(f\"✓ Checkpoint loaded successfully\")\n",
    "\n",
    "# Run inference on test set\n",
    "preds = []\n",
    "trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_x_mark, batch_y_mark in test_loader:\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        \n",
    "        # Model forward takes only x (input sequence)\n",
    "        outputs = model(batch_x)\n",
    "        preds.append(outputs.cpu().numpy())\n",
    "        \n",
    "        # Extract only the last pred_len timesteps from batch_y\n",
    "        batch_y_target = batch_y[:, -args.pred_len:, :]\n",
    "        trues.append(batch_y_target.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "trues = np.concatenate(trues, axis=0)\n",
    "print(f\"\\n✓ Inference complete\")\n",
    "print(f\"  Predictions shape: {preds.shape}\")\n",
    "print(f\"  Ground truth shape: {trues.shape}\")\n",
    "\n",
    "# Calculate metrics on TARGET features only\n",
    "preds_target = preds[:, :, target_indices]\n",
    "trues_target = trues[:, :, target_indices]\n",
    "\n",
    "# Convert to torch tensors and compute MSE (same as training loss)\n",
    "preds_torch = torch.from_numpy(preds_target).float()\n",
    "trues_torch = torch.from_numpy(trues_target).float()\n",
    "mse_criterion = nn.MSELoss()\n",
    "mse = mse_criterion(preds_torch, trues_torch).item()\n",
    "mae = np.mean(np.abs(preds_target - trues_target))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"EVALUATION RESULTS (Target Features Only)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Test Samples: {len(preds)}\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  MAE: {mae:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mse):.6f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5901a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check what the model actually outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL OUTPUT VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model c_out (expected output channels): {args.c_out}\")\n",
    "print(f\"Target indices from config: {target_indices}\")\n",
    "print(f\"Target names from config: {target_names}\")\n",
    "print(f\"\\nActual predictions shape: {preds.shape}\")\n",
    "print(f\"Actual ground truth shape: {trues.shape}\")\n",
    "print(f\"\\nMismatch detected!\" if preds.shape[2] != trues.shape[2] else \"Shapes match\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b24aa6",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9635e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Features to plot\n",
    "features_to_plot = [\n",
    "    'p (mbar)',           # air pressure\n",
    "    'T (degC)',           # temperature\n",
    "    'wv (m/s)',           # wind speed\n",
    "    'max. wv (m/s)',      # maximum wind speed\n",
    "    'rain (mm)',          # rainfall amount\n",
    "    'raining (s)'         # rainfall duration\n",
    "]\n",
    "\n",
    "# Full 22-feature names (22 features in ground truth: 20 weather + 2 hour)\n",
    "full_feature_names = [\n",
    "    'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', \n",
    "    'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', \n",
    "    'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', \n",
    "    'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m²)', \n",
    "    'PAR (µmol/m²/s)', 'max. PAR (µmol/m²/s)', 'Tlog (degC)', 'hour_sin', 'hour_cos'\n",
    "]\n",
    "\n",
    "# Model only predicts these 7 target features at indices: [0, 1, 11, 12, 14, 15]\n",
    "# Map predicted feature indices to their position in the 7-feature prediction array\n",
    "pred_feature_map = {\n",
    "    0: 0,   # p (mbar) -> pred index 0\n",
    "    1: 1,   # T (degC) -> pred index 1  \n",
    "    11: 2,  # wv (m/s) -> pred index 2\n",
    "    12: 3,  # max. wv (m/s) -> pred index 3\n",
    "    14: 4,  # rain (mm) -> pred index 4\n",
    "    15: 5   # raining (s) -> pred index 5\n",
    "}\n",
    "\n",
    "# Select a random sample using seed 2021\n",
    "np.random.seed(2021)\n",
    "sample_idx = np.random.randint(0, len(preds))\n",
    "\n",
    "# Get sample prediction (7 features) and ground truth (22 features)\n",
    "sample_pred = preds[sample_idx]  # Shape: (pred_len, 7)\n",
    "sample_true_full = trues[sample_idx]  # Shape: (pred_len, 22)\n",
    "\n",
    "print(f\"Sample {sample_idx} shapes: pred={sample_pred.shape}, true={sample_true_full.shape}\")\n",
    "print(f\"Target indices (in 22-feature space): {target_indices}\")\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, len(features_to_plot), figsize=(24, 4))\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature_name in enumerate(features_to_plot):\n",
    "    # Get index in the full 22-feature ground truth\n",
    "    gt_idx = full_feature_names.index(feature_name)\n",
    "    \n",
    "    # Get index in the 7-feature prediction array\n",
    "    if gt_idx in pred_feature_map:\n",
    "        pred_idx = pred_feature_map[gt_idx]\n",
    "        \n",
    "        ax = axes[i]\n",
    "        time_steps = np.arange(len(sample_pred))\n",
    "        \n",
    "        # Plot ground truth from full 22-feature array\n",
    "        ax.plot(time_steps, sample_true_full[:, gt_idx], label='Ground Truth', \n",
    "                linewidth=2, color='blue', alpha=0.7)\n",
    "        \n",
    "        # Plot prediction from 7-feature prediction array\n",
    "        ax.plot(time_steps, sample_pred[:, pred_idx], label='Prediction', \n",
    "                linewidth=2, color='red', alpha=0.7, linestyle='--')\n",
    "        \n",
    "        ax.set_title(feature_name, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time Step', fontsize=10)\n",
    "        ax.set_ylabel('Value', fontsize=10)\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        print(f\"Warning: {feature_name} not in predicted features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Weather Forecast Predictions (Sample {sample_idx}, Horizon: {args.pred_len} steps)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nPlotted sample index: {sample_idx}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
