{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b5141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content/PatchTST\n",
      "PyTorch Version: 2.8.0+cu126\n",
      "CUDA Available: True\n",
      "Python path head: ['/content/PatchTST/PatchTST_supervised', '/content/PatchTST/PatchTST_physics_integrated', '/content/PatchTST', '/', '/env/python']\n",
      "NumPy compatibility patch applied for np.Inf -> np.inf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set repository root path and change to it\n",
    "repo_root_path = '/content/PatchTST'\n",
    "os.chdir(repo_root_path)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Build clean sys.path with supervised ahead of physics to avoid utils shadowing\n",
    "supervised_path = os.path.join(repo_root_path, 'PatchTST_supervised')\n",
    "physics_path = os.path.join(repo_root_path, 'PatchTST_physics_integrated')\n",
    "new_paths = [p for p in [supervised_path, physics_path, repo_root_path] if p not in sys.path]\n",
    "sys.path = new_paths + sys.path  # prepend in desired order\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Python path head: {sys.path[:5]}\")\n",
    "\n",
    "# Numpy fix\n",
    "if not hasattr(np, 'Inf'):\n",
    "    np.Inf = np.inf\n",
    "    np.NaN = np.nan\n",
    "    np.NAN = np.nan\n",
    "    np.NINF = np.NINF if hasattr(np, 'NINF') else -np.inf\n",
    "    print(\"NumPy compatibility patch applied for np.Inf -> np.inf\")\n",
    "else:\n",
    "    print(\"NumPy already has np.Inf attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb06a0",
   "metadata": {},
   "source": [
    "## 1. Import Physics-Integrated PatchTST Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad84d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "from PatchTST_physics_integrated.config import PhysicsIntegratedConfig\n",
    "from PatchTST_physics_integrated.models import PhysicsIntegratedPatchTST\n",
    "from PatchTST_physics_integrated.training_utils import set_seed, get_target_indices, get_scheduler\n",
    "from PatchTST_physics_integrated.trainer import train_model\n",
    "from PatchTST_physics_integrated.evaluation import evaluate_model, evaluate_per_channel\n",
    "from PatchTST_physics_integrated.data_preprocessing import add_hour_of_day_features\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8b2fa",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2634f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n",
      "\n",
      "Configuration:\n",
      "  Input channels: 23\n",
      "  Output channels: 21\n",
      "  Sequence length: 512\n",
      "  Prediction length: 336\n",
      "  Batch size: 32\n",
      "  Learning rate: 0.0001\n",
      "\n",
      "Channel Groups (sources → targets):\n",
      "  rain_predictors:\n",
      "    Sources: rain (mm), raining (s), rh (%), Tdew (degC), H2OC (mmol/mol), sh (g/kg), VPact (mbar), VPmax (mbar), VPdef (mbar), p (mbar), Tpot (K), rho (g/m**3), wv (m/s), max. wv (m/s), wd (deg), PAR (umol/m2/s), hour_sin, hour_cos\n",
      "    Targets: rain (mm), raining (s)\n",
      "  temperature_predictors:\n",
      "    Sources: T (degC), Tpot (K), Tdew (degC), PAR (umol/m2/s), p (mbar), rho (g/m**3), wv (m/s), max. wv (m/s), wd (deg), rh (%), sh (g/kg), H2OC (mmol/mol), VPact (mbar), VPmax (mbar), VPdef (mbar), hour_sin, hour_cos\n",
      "    Targets: T (degC), Tpot (K), Tdew (degC)\n",
      "  wind_predictors:\n",
      "    Sources: wv (m/s), max. wv (m/s), p (mbar), Tpot (K), T (degC), Tdew (degC), PAR (umol/m2/s), rh (%), sh (g/kg), H2OC (mmol/mol), VPact (mbar), VPmax (mbar), VPdef (mbar), rho (g/m**3), wd (deg), hour_sin, hour_cos\n",
      "    Targets: wv (m/s), max. wv (m/s)\n",
      "  other_variables:\n",
      "    Sources: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC), T (degC), Tpot (K), Tdew (degC), wv (m/s), max. wv (m/s), hour_sin, hour_cos\n",
      "    Targets: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC)\n"
     ]
    }
   ],
   "source": [
    "# Create configuration\n",
    "args = PhysicsIntegratedConfig()\n",
    "set_seed(args.random_seed)\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Input channels: {args.enc_in}\")\n",
    "print(f\"  Output channels: {args.c_out}\")\n",
    "print(f\"  Sequence length: {args.seq_len}\")\n",
    "print(f\"  Prediction length: {args.pred_len}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"\\nChannel Groups (sources → targets):\")\n",
    "for name, group in args.channel_groups.items():\n",
    "    indices = group['indices']\n",
    "    src_names = group.get('names', [])\n",
    "    output_indices = set(group.get('output_indices', []))\n",
    "    # Map output indices to names using the indices ordering\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(indices) if idx in output_indices] if src_names else []\n",
    "    print(f\"  {name}:\")\n",
    "    if src_names:\n",
    "        print(f\"    Sources: {', '.join(src_names)}\")\n",
    "    else:\n",
    "        print(f\"    Sources: (names not provided)\")\n",
    "    if tgt_names:\n",
    "        print(f\"    Targets: {', '.join(tgt_names)}\")\n",
    "    else:\n",
    "        print(f\"    Targets: (names not provided)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11772ef",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data (Add Hour Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ca1c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset from: ./datasets/weather/weather.csv\n",
      "Original shape: (52696, 22)\n",
      "Original columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT']\n",
      "\n",
      "New shape: (52696, 24)\n",
      "New columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT', 'hour_sin', 'hour_cos']\n",
      "\n",
      "Hour feature statistics:\n",
      "           hour_sin      hour_cos\n",
      "count  52696.000000  5.269600e+04\n",
      "mean      -0.000070  1.437736e-04\n",
      "std        0.707146  7.070808e-01\n",
      "min       -1.000000 -1.000000e+00\n",
      "25%       -0.707107 -7.071068e-01\n",
      "50%        0.000000  6.123234e-17\n",
      "75%        0.707107  7.071068e-01\n",
      "max        1.000000  1.000000e+00\n",
      "\n",
      "New shape: (52696, 24)\n",
      "New columns: ['date', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)', 'rain (mm)', 'raining (s)', 'SWDR (W/m�)', 'PAR (�mol/m�/s)', 'max. PAR (�mol/m�/s)', 'Tlog (degC)', 'OT', 'hour_sin', 'hour_cos']\n",
      "\n",
      "Hour feature statistics:\n",
      "           hour_sin      hour_cos\n",
      "count  52696.000000  5.269600e+04\n",
      "mean      -0.000070  1.437736e-04\n",
      "std        0.707146  7.070808e-01\n",
      "min       -1.000000 -1.000000e+00\n",
      "25%       -0.707107 -7.071068e-01\n",
      "50%        0.000000  6.123234e-17\n",
      "75%        0.707107  7.071068e-01\n",
      "max        1.000000  1.000000e+00\n",
      "\n",
      "✓ Enhanced dataset saved to: ./datasets/weather/weather_with_hour.csv\n",
      "\n",
      "✓ Enhanced dataset saved to: ./datasets/weather/weather_with_hour.csv\n"
     ]
    }
   ],
   "source": [
    "# Add hour-of-day features to dataset\n",
    "original_path = os.path.join(args.root_path, 'weather.csv')\n",
    "enhanced_path = os.path.join(args.root_path, args.data_path)\n",
    "\n",
    "df_enhanced = add_hour_of_day_features(original_path, enhanced_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660e01f",
   "metadata": {},
   "source": [
    "## 4. Load Data (Using PatchTST Data Providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310d337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to: /content/PatchTST/PatchTST_supervised\n",
      "train 36040\n",
      "val 4935\n",
      "train 36040\n",
      "val 4935\n",
      "test 10204\n",
      "\n",
      "Data loaded:\n",
      "  Train samples: 36040\n",
      "  Val samples: 4935\n",
      "  Test samples: 10204\n",
      "test 10204\n",
      "\n",
      "Data loaded:\n",
      "  Train samples: 36040\n",
      "  Val samples: 4935\n",
      "  Test samples: 10204\n"
     ]
    }
   ],
   "source": [
    "# Change to PatchTST_supervised directory for data_provider imports\n",
    "import importlib\n",
    "\n",
    "os.chdir(os.path.join(repo_root_path, 'PatchTST_supervised'))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Clear cached modules to avoid stale 'utils' shadowing\n",
    "for m in [\n",
    "    'utils', 'utils.timefeatures',\n",
    "    'data_provider', 'data_provider.data_loader', 'data_provider.data_factory'\n",
    "]:\n",
    "    if m in sys.modules:\n",
    "        sys.modules.pop(m, None)\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "\n",
    "os.chdir(repo_root_path)\n",
    "\n",
    "# Create data loaders\n",
    "train_data, train_loader = data_provider(args, 'train')\n",
    "val_data, val_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Val samples: {len(val_data)}\")\n",
    "print(f\"  Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907b34",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7647f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model created:\n",
      "  Total parameters: 37,195,538\n",
      "  Target variables by group:\n",
      "    rain_predictors: rain (mm), raining (s)\n",
      "    temperature_predictors: T (degC), Tpot (K), Tdew (degC)\n",
      "    wind_predictors: wv (m/s), max. wv (m/s)\n",
      "    other_variables: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC)\n",
      "\n",
      "Model created:\n",
      "  Total parameters: 37,195,538\n",
      "  Target variables by group:\n",
      "    rain_predictors: rain (mm), raining (s)\n",
      "    temperature_predictors: T (degC), Tpot (K), Tdew (degC)\n",
      "    wind_predictors: wv (m/s), max. wv (m/s)\n",
      "    other_variables: p (mbar), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), wd (deg), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), Tlog (degC), CO2 (ppm), PAR (umol/m2/s), Tmax (degC), Tmin (degC)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = PhysicsIntegratedPatchTST(args).float().to(device)\n",
    "\n",
    "# Get target indices and names\n",
    "target_indices, target_names = get_target_indices(args.channel_groups)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"  Target variables by group:\")\n",
    "for group, info in args.channel_groups.items():\n",
    "    output_indices = set(info.get('output_indices', []))\n",
    "    src_names = info.get('names', [])\n",
    "    tgt_names = [src_names[i] for i, idx in enumerate(info['indices']) if idx in output_indices] if src_names else []\n",
    "    print(f\"    {group}: {', '.join(tgt_names) if tgt_names else '(names not provided)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ebf75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture:\n",
      "\n",
      "PhysicsIntegratedPatchTST(\n",
      "  (revin_layer): RevIN()\n",
      "  (encoders): ModuleDict(\n",
      "    (rain_predictors): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 12))\n",
      "      (W_P): Linear(in_features=24, out_features=128, bias=True)\n",
      "      (deep_ffn): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (dropout2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (heads): ModuleList(\n",
      "        (0-1): 2 x Sequential(\n",
      "          (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "          (1): Linear(in_features=5376, out_features=336, bias=True)\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (temperature_predictors): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 18))\n",
      "      (W_P): Linear(in_features=36, out_features=128, bias=True)\n",
      "      (deep_ffn): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (dropout2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (heads): ModuleList(\n",
      "        (0-2): 3 x Sequential(\n",
      "          (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "          (1): Linear(in_features=3584, out_features=336, bias=True)\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (wind_predictors): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 16))\n",
      "      (W_P): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (deep_ffn): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (dropout2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (heads): ModuleList(\n",
      "        (0-1): 2 x Sequential(\n",
      "          (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "          (1): Linear(in_features=4096, out_features=336, bias=True)\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (other_variables): PerChannelEncoder(\n",
      "      (padding_layer): ReplicationPad1d((0, 12))\n",
      "      (W_P): Linear(in_features=24, out_features=128, bias=True)\n",
      "      (deep_ffn): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.2, inplace=False)\n",
      "            (dropout2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (heads): ModuleList(\n",
      "        (0-13): 14 x Sequential(\n",
      "          (0): Flatten(start_dim=-2, end_dim=-1)\n",
      "          (1): Linear(in_features=5376, out_features=336, bias=True)\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (cross_group_attn): CrossGroupAttention(\n",
      "    (channel_proj): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (cross_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (ffn): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_proj): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Parameter counts by layer:\n",
      "cross_group_weight: 1 params\n",
      "encoders.rain_predictors.W_pos: 5376 params\n",
      "encoders.rain_predictors.W_P.weight: 3072 params\n",
      "encoders.rain_predictors.W_P.bias: 128 params\n",
      "encoders.rain_predictors.deep_ffn.0.weight: 32768 params\n",
      "encoders.rain_predictors.deep_ffn.0.bias: 256 params\n",
      "encoders.rain_predictors.deep_ffn.3.weight: 32768 params\n",
      "encoders.rain_predictors.deep_ffn.3.bias: 128 params\n",
      "encoders.rain_predictors.ffn_norm.weight: 128 params\n",
      "encoders.rain_predictors.ffn_norm.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.0.self_attn.in_proj_weight: 49152 params\n",
      "encoders.rain_predictors.encoder.layers.0.self_attn.in_proj_bias: 384 params\n",
      "encoders.rain_predictors.encoder.layers.0.self_attn.out_proj.weight: 16384 params\n",
      "encoders.rain_predictors.encoder.layers.0.self_attn.out_proj.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.0.linear1.weight: 32768 params\n",
      "encoders.rain_predictors.encoder.layers.0.linear1.bias: 256 params\n",
      "encoders.rain_predictors.encoder.layers.0.linear2.weight: 32768 params\n",
      "encoders.rain_predictors.encoder.layers.0.linear2.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.0.norm1.weight: 128 params\n",
      "encoders.rain_predictors.encoder.layers.0.norm1.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.0.norm2.weight: 128 params\n",
      "encoders.rain_predictors.encoder.layers.0.norm2.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.1.self_attn.in_proj_weight: 49152 params\n",
      "encoders.rain_predictors.encoder.layers.1.self_attn.in_proj_bias: 384 params\n",
      "encoders.rain_predictors.encoder.layers.1.self_attn.out_proj.weight: 16384 params\n",
      "encoders.rain_predictors.encoder.layers.1.self_attn.out_proj.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.1.linear1.weight: 32768 params\n",
      "encoders.rain_predictors.encoder.layers.1.linear1.bias: 256 params\n",
      "encoders.rain_predictors.encoder.layers.1.linear2.weight: 32768 params\n",
      "encoders.rain_predictors.encoder.layers.1.linear2.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.1.norm1.weight: 128 params\n",
      "encoders.rain_predictors.encoder.layers.1.norm1.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.1.norm2.weight: 128 params\n",
      "encoders.rain_predictors.encoder.layers.1.norm2.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.2.self_attn.in_proj_weight: 49152 params\n",
      "encoders.rain_predictors.encoder.layers.2.self_attn.in_proj_bias: 384 params\n",
      "encoders.rain_predictors.encoder.layers.2.self_attn.out_proj.weight: 16384 params\n",
      "encoders.rain_predictors.encoder.layers.2.self_attn.out_proj.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.2.linear1.weight: 32768 params\n",
      "encoders.rain_predictors.encoder.layers.2.linear1.bias: 256 params\n",
      "encoders.rain_predictors.encoder.layers.2.linear2.weight: 32768 params\n",
      "encoders.rain_predictors.encoder.layers.2.linear2.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.2.norm1.weight: 128 params\n",
      "encoders.rain_predictors.encoder.layers.2.norm1.bias: 128 params\n",
      "encoders.rain_predictors.encoder.layers.2.norm2.weight: 128 params\n",
      "encoders.rain_predictors.encoder.layers.2.norm2.bias: 128 params\n",
      "encoders.rain_predictors.heads.0.1.weight: 1806336 params\n",
      "encoders.rain_predictors.heads.0.1.bias: 336 params\n",
      "encoders.rain_predictors.heads.1.1.weight: 1806336 params\n",
      "encoders.rain_predictors.heads.1.1.bias: 336 params\n",
      "encoders.temperature_predictors.W_pos: 3584 params\n",
      "encoders.temperature_predictors.W_P.weight: 4608 params\n",
      "encoders.temperature_predictors.W_P.bias: 128 params\n",
      "encoders.temperature_predictors.deep_ffn.0.weight: 32768 params\n",
      "encoders.temperature_predictors.deep_ffn.0.bias: 256 params\n",
      "encoders.temperature_predictors.deep_ffn.3.weight: 32768 params\n",
      "encoders.temperature_predictors.deep_ffn.3.bias: 128 params\n",
      "encoders.temperature_predictors.ffn_norm.weight: 128 params\n",
      "encoders.temperature_predictors.ffn_norm.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.0.self_attn.in_proj_weight: 49152 params\n",
      "encoders.temperature_predictors.encoder.layers.0.self_attn.in_proj_bias: 384 params\n",
      "encoders.temperature_predictors.encoder.layers.0.self_attn.out_proj.weight: 16384 params\n",
      "encoders.temperature_predictors.encoder.layers.0.self_attn.out_proj.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.0.linear1.weight: 32768 params\n",
      "encoders.temperature_predictors.encoder.layers.0.linear1.bias: 256 params\n",
      "encoders.temperature_predictors.encoder.layers.0.linear2.weight: 32768 params\n",
      "encoders.temperature_predictors.encoder.layers.0.linear2.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.0.norm1.weight: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.0.norm1.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.0.norm2.weight: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.0.norm2.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.1.self_attn.in_proj_weight: 49152 params\n",
      "encoders.temperature_predictors.encoder.layers.1.self_attn.in_proj_bias: 384 params\n",
      "encoders.temperature_predictors.encoder.layers.1.self_attn.out_proj.weight: 16384 params\n",
      "encoders.temperature_predictors.encoder.layers.1.self_attn.out_proj.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.1.linear1.weight: 32768 params\n",
      "encoders.temperature_predictors.encoder.layers.1.linear1.bias: 256 params\n",
      "encoders.temperature_predictors.encoder.layers.1.linear2.weight: 32768 params\n",
      "encoders.temperature_predictors.encoder.layers.1.linear2.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.1.norm1.weight: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.1.norm1.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.1.norm2.weight: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.1.norm2.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.2.self_attn.in_proj_weight: 49152 params\n",
      "encoders.temperature_predictors.encoder.layers.2.self_attn.in_proj_bias: 384 params\n",
      "encoders.temperature_predictors.encoder.layers.2.self_attn.out_proj.weight: 16384 params\n",
      "encoders.temperature_predictors.encoder.layers.2.self_attn.out_proj.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.2.linear1.weight: 32768 params\n",
      "encoders.temperature_predictors.encoder.layers.2.linear1.bias: 256 params\n",
      "encoders.temperature_predictors.encoder.layers.2.linear2.weight: 32768 params\n",
      "encoders.temperature_predictors.encoder.layers.2.linear2.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.2.norm1.weight: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.2.norm1.bias: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.2.norm2.weight: 128 params\n",
      "encoders.temperature_predictors.encoder.layers.2.norm2.bias: 128 params\n",
      "encoders.temperature_predictors.heads.0.1.weight: 1204224 params\n",
      "encoders.temperature_predictors.heads.0.1.bias: 336 params\n",
      "encoders.temperature_predictors.heads.1.1.weight: 1204224 params\n",
      "encoders.temperature_predictors.heads.1.1.bias: 336 params\n",
      "encoders.temperature_predictors.heads.2.1.weight: 1204224 params\n",
      "encoders.temperature_predictors.heads.2.1.bias: 336 params\n",
      "encoders.wind_predictors.W_pos: 4096 params\n",
      "encoders.wind_predictors.W_P.weight: 4096 params\n",
      "encoders.wind_predictors.W_P.bias: 128 params\n",
      "encoders.wind_predictors.deep_ffn.0.weight: 32768 params\n",
      "encoders.wind_predictors.deep_ffn.0.bias: 256 params\n",
      "encoders.wind_predictors.deep_ffn.3.weight: 32768 params\n",
      "encoders.wind_predictors.deep_ffn.3.bias: 128 params\n",
      "encoders.wind_predictors.ffn_norm.weight: 128 params\n",
      "encoders.wind_predictors.ffn_norm.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.0.self_attn.in_proj_weight: 49152 params\n",
      "encoders.wind_predictors.encoder.layers.0.self_attn.in_proj_bias: 384 params\n",
      "encoders.wind_predictors.encoder.layers.0.self_attn.out_proj.weight: 16384 params\n",
      "encoders.wind_predictors.encoder.layers.0.self_attn.out_proj.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.0.linear1.weight: 32768 params\n",
      "encoders.wind_predictors.encoder.layers.0.linear1.bias: 256 params\n",
      "encoders.wind_predictors.encoder.layers.0.linear2.weight: 32768 params\n",
      "encoders.wind_predictors.encoder.layers.0.linear2.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.0.norm1.weight: 128 params\n",
      "encoders.wind_predictors.encoder.layers.0.norm1.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.0.norm2.weight: 128 params\n",
      "encoders.wind_predictors.encoder.layers.0.norm2.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.1.self_attn.in_proj_weight: 49152 params\n",
      "encoders.wind_predictors.encoder.layers.1.self_attn.in_proj_bias: 384 params\n",
      "encoders.wind_predictors.encoder.layers.1.self_attn.out_proj.weight: 16384 params\n",
      "encoders.wind_predictors.encoder.layers.1.self_attn.out_proj.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.1.linear1.weight: 32768 params\n",
      "encoders.wind_predictors.encoder.layers.1.linear1.bias: 256 params\n",
      "encoders.wind_predictors.encoder.layers.1.linear2.weight: 32768 params\n",
      "encoders.wind_predictors.encoder.layers.1.linear2.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.1.norm1.weight: 128 params\n",
      "encoders.wind_predictors.encoder.layers.1.norm1.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.1.norm2.weight: 128 params\n",
      "encoders.wind_predictors.encoder.layers.1.norm2.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.2.self_attn.in_proj_weight: 49152 params\n",
      "encoders.wind_predictors.encoder.layers.2.self_attn.in_proj_bias: 384 params\n",
      "encoders.wind_predictors.encoder.layers.2.self_attn.out_proj.weight: 16384 params\n",
      "encoders.wind_predictors.encoder.layers.2.self_attn.out_proj.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.2.linear1.weight: 32768 params\n",
      "encoders.wind_predictors.encoder.layers.2.linear1.bias: 256 params\n",
      "encoders.wind_predictors.encoder.layers.2.linear2.weight: 32768 params\n",
      "encoders.wind_predictors.encoder.layers.2.linear2.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.2.norm1.weight: 128 params\n",
      "encoders.wind_predictors.encoder.layers.2.norm1.bias: 128 params\n",
      "encoders.wind_predictors.encoder.layers.2.norm2.weight: 128 params\n",
      "encoders.wind_predictors.encoder.layers.2.norm2.bias: 128 params\n",
      "encoders.wind_predictors.heads.0.1.weight: 1376256 params\n",
      "encoders.wind_predictors.heads.0.1.bias: 336 params\n",
      "encoders.wind_predictors.heads.1.1.weight: 1376256 params\n",
      "encoders.wind_predictors.heads.1.1.bias: 336 params\n",
      "encoders.other_variables.W_pos: 5376 params\n",
      "encoders.other_variables.W_P.weight: 3072 params\n",
      "encoders.other_variables.W_P.bias: 128 params\n",
      "encoders.other_variables.deep_ffn.0.weight: 32768 params\n",
      "encoders.other_variables.deep_ffn.0.bias: 256 params\n",
      "encoders.other_variables.deep_ffn.3.weight: 32768 params\n",
      "encoders.other_variables.deep_ffn.3.bias: 128 params\n",
      "encoders.other_variables.ffn_norm.weight: 128 params\n",
      "encoders.other_variables.ffn_norm.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.0.self_attn.in_proj_weight: 49152 params\n",
      "encoders.other_variables.encoder.layers.0.self_attn.in_proj_bias: 384 params\n",
      "encoders.other_variables.encoder.layers.0.self_attn.out_proj.weight: 16384 params\n",
      "encoders.other_variables.encoder.layers.0.self_attn.out_proj.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.0.linear1.weight: 32768 params\n",
      "encoders.other_variables.encoder.layers.0.linear1.bias: 256 params\n",
      "encoders.other_variables.encoder.layers.0.linear2.weight: 32768 params\n",
      "encoders.other_variables.encoder.layers.0.linear2.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.0.norm1.weight: 128 params\n",
      "encoders.other_variables.encoder.layers.0.norm1.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.0.norm2.weight: 128 params\n",
      "encoders.other_variables.encoder.layers.0.norm2.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.1.self_attn.in_proj_weight: 49152 params\n",
      "encoders.other_variables.encoder.layers.1.self_attn.in_proj_bias: 384 params\n",
      "encoders.other_variables.encoder.layers.1.self_attn.out_proj.weight: 16384 params\n",
      "encoders.other_variables.encoder.layers.1.self_attn.out_proj.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.1.linear1.weight: 32768 params\n",
      "encoders.other_variables.encoder.layers.1.linear1.bias: 256 params\n",
      "encoders.other_variables.encoder.layers.1.linear2.weight: 32768 params\n",
      "encoders.other_variables.encoder.layers.1.linear2.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.1.norm1.weight: 128 params\n",
      "encoders.other_variables.encoder.layers.1.norm1.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.1.norm2.weight: 128 params\n",
      "encoders.other_variables.encoder.layers.1.norm2.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.2.self_attn.in_proj_weight: 49152 params\n",
      "encoders.other_variables.encoder.layers.2.self_attn.in_proj_bias: 384 params\n",
      "encoders.other_variables.encoder.layers.2.self_attn.out_proj.weight: 16384 params\n",
      "encoders.other_variables.encoder.layers.2.self_attn.out_proj.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.2.linear1.weight: 32768 params\n",
      "encoders.other_variables.encoder.layers.2.linear1.bias: 256 params\n",
      "encoders.other_variables.encoder.layers.2.linear2.weight: 32768 params\n",
      "encoders.other_variables.encoder.layers.2.linear2.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.2.norm1.weight: 128 params\n",
      "encoders.other_variables.encoder.layers.2.norm1.bias: 128 params\n",
      "encoders.other_variables.encoder.layers.2.norm2.weight: 128 params\n",
      "encoders.other_variables.encoder.layers.2.norm2.bias: 128 params\n",
      "encoders.other_variables.heads.0.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.0.1.bias: 336 params\n",
      "encoders.other_variables.heads.1.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.1.1.bias: 336 params\n",
      "encoders.other_variables.heads.2.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.2.1.bias: 336 params\n",
      "encoders.other_variables.heads.3.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.3.1.bias: 336 params\n",
      "encoders.other_variables.heads.4.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.4.1.bias: 336 params\n",
      "encoders.other_variables.heads.5.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.5.1.bias: 336 params\n",
      "encoders.other_variables.heads.6.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.6.1.bias: 336 params\n",
      "encoders.other_variables.heads.7.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.7.1.bias: 336 params\n",
      "encoders.other_variables.heads.8.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.8.1.bias: 336 params\n",
      "encoders.other_variables.heads.9.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.9.1.bias: 336 params\n",
      "encoders.other_variables.heads.10.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.10.1.bias: 336 params\n",
      "encoders.other_variables.heads.11.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.11.1.bias: 336 params\n",
      "encoders.other_variables.heads.12.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.12.1.bias: 336 params\n",
      "encoders.other_variables.heads.13.1.weight: 1806336 params\n",
      "encoders.other_variables.heads.13.1.bias: 336 params\n",
      "cross_group_attn.channel_proj.weight: 64 params\n",
      "cross_group_attn.channel_proj.bias: 64 params\n",
      "cross_group_attn.norm1.weight: 64 params\n",
      "cross_group_attn.norm1.bias: 64 params\n",
      "cross_group_attn.cross_attn.in_proj_weight: 12288 params\n",
      "cross_group_attn.cross_attn.in_proj_bias: 192 params\n",
      "cross_group_attn.cross_attn.out_proj.weight: 4096 params\n",
      "cross_group_attn.cross_attn.out_proj.bias: 64 params\n",
      "cross_group_attn.ffn.0.weight: 8192 params\n",
      "cross_group_attn.ffn.0.bias: 128 params\n",
      "cross_group_attn.ffn.3.weight: 8192 params\n",
      "cross_group_attn.ffn.3.bias: 64 params\n",
      "cross_group_attn.norm2.weight: 64 params\n",
      "cross_group_attn.norm2.bias: 64 params\n",
      "cross_group_attn.output_proj.weight: 64 params\n",
      "cross_group_attn.output_proj.bias: 1 params\n"
     ]
    }
   ],
   "source": [
    "# Inspect full model architecture\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "print(model)\n",
    "\n",
    "# Optional: count parameters by module\n",
    "param_lines = []\n",
    "for name, param in model.named_parameters():\n",
    "    param_lines.append(f\"{name}: {param.numel()} params\")\n",
    "print(\"\\nParameter counts by layer:\")\n",
    "print(\"\\n\".join(param_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1e7e1",
   "metadata": {},
   "source": [
    "## 6. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e589083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = get_scheduler(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_loader) * args.train_epochs\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_path = args.checkpoints\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061ffe7",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232779d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Physics-Integrated PatchTST Training...\n",
      "Checkpoint path: ./checkpoints_physics_hour\n",
      "======================================================================\n",
      "Input channels: 23 (21 weather + 2 hour)\n",
      "Output channels: 21 (21 weather only)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Physics Groups (with integrated hour features):\n",
      "  rain_predictors: 2 weather + hour → patch=24 , stride=12\n",
      "  temperature_predictors: 3 weather + hour → patch=36 , stride=18\n",
      "  wind_predictors: 2 weather + hour → patch=32 , stride=16\n",
      "  other_variables: 14 weather + hour → patch=24 , stride=12\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-499576749.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = train_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/PatchTST/PatchTST_physics_integrated/trainer.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, test_loader, optimizer, scheduler, criterion, args, device, target_indices, checkpoint_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Target - only selected indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/PatchTST/PatchTST_physics_integrated/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;31m# Encode - returns only specified output channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mgroup_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# Place outputs in correct positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/PatchTST/PatchTST_physics_integrated/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, output_channel_mask)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# Transformer encoder (channel-independent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mch_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch_x\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [bs, patch_num, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# Only create output for weather channels, not hour features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    933\u001b[0m             x = self.norm1(\n\u001b[1;32m    934\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                 \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m             )\n\u001b[1;32m    937\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mis_causal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     ) -> Tensor:\n\u001b[0;32m--> 949\u001b[0;31m         x = self.self_attn(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             )\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1381\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6368\u001b[0m     \u001b[0;31m# reshape q, k, v for multihead attention and make them batch first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6369\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6370\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatic_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6372\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    target_indices=target_indices,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aefda7",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c21149",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall losses\n",
    "axes[0].plot(history['train_losses'], label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(history['val_losses'], label='Validation Loss', marker='s', markersize=3)\n",
    "axes[0].plot(history['test_losses'], label='Test Loss', marker='^', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Target variable losses\n",
    "for target_name, losses in history['target_variable_losses'].items():\n",
    "    axes[1].plot(losses, label=target_name.capitalize(), marker='o', markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Target Variable Losses')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a637c",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6917189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = evaluate_model(model, test_loader, device, args)\n",
    "\n",
    "# Get per-channel metrics\n",
    "per_channel_metrics = evaluate_per_channel(\n",
    "    results['preds'],\n",
    "    results['trues'],\n",
    "    target_indices,\n",
    "    target_names\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Channel Metrics:\")\n",
    "for ch_name, metrics in per_channel_metrics.items():\n",
    "    print(f\"  {ch_name}:\")\n",
    "    print(f\"    MSE: {metrics['mse']:.7f}\")\n",
    "    print(f\"    MAE: {metrics['mae']:.7f}\")\n",
    "    print(f\"    RMSE: {metrics['rmse']:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa170ea3",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9635e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for target variables\n",
    "num_samples = 3\n",
    "fig, axes = plt.subplots(len(target_indices), num_samples, figsize=(5*num_samples, 4*len(target_indices)))\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i, (ch_idx, ch_name) in enumerate(zip(target_indices, target_names)):\n",
    "    for j in range(num_samples):\n",
    "        input_seq = results['inputs'][j, :, ch_idx]\n",
    "        true_seq = results['trues'][j, :, ch_idx]\n",
    "        pred_seq = results['preds'][j, :, ch_idx]\n",
    "        \n",
    "        input_steps = np.arange(len(input_seq))\n",
    "        pred_steps = np.arange(len(input_seq), len(input_seq) + len(pred_seq))\n",
    "        \n",
    "        ax = axes[i, j]\n",
    "        ax.plot(input_steps, input_seq, 'b-', label='Input')\n",
    "        ax.plot(pred_steps, true_seq, 'g-', label='Ground Truth')\n",
    "        ax.plot(pred_steps, pred_seq, 'r--', label='Prediction')\n",
    "        ax.axvline(x=len(input_seq)-1, color='gray', linestyle=':', linewidth=1.5)\n",
    "        ax.set_title(f'Sample {j+1} - {ch_name}')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
